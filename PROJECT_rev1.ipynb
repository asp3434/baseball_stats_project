{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "www.baseball-reference.com only allows 20 requests per minute. If we violate this rule, our session will be locked out for one hour."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request website for each year\n",
    "main_url = 'https://www.baseball-reference.com'\n",
    "years = ['2019', '2021', '2022']\n",
    "pages = []\n",
    "soups = []\n",
    "for i in range(len(years)):\n",
    "    url = f'{main_url}/leagues/majors/{years[i]}.shtml'\n",
    "    page = requests.get(url)\n",
    "    pages.append(page)\n",
    "    soups.append(BeautifulSoup(page.content))\n",
    "    \n",
    "# three requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through soups to get the winning team links\n",
    "pattern = '/teams/[A-Z][A-Z][A-Z]/20[0-9][0-9].shtml'\n",
    "winning_team_links = []\n",
    "for k in range(len(soups)):\n",
    "    hrefs = soups[k].find_all('a', href=True)\n",
    "    for i in range(len(hrefs)):\n",
    "        new_str = str(hrefs[i])\n",
    "        check = re.search(pattern, new_str)\n",
    "        if check:\n",
    "            link = new_str[9:30]  \n",
    "            break\n",
    "    winning_team_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request website for each winning team\n",
    "win_team_pages = []\n",
    "win_team_soups = []\n",
    "for i in range(len(winning_team_links)):\n",
    "    url = f'{main_url}{winning_team_links[i]}'\n",
    "    page = requests.get(url)\n",
    "    win_team_pages.append(page)\n",
    "    win_team_soups.append(BeautifulSoup(page.content))\n",
    "    \n",
    "# three requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through table to get link for each player of each team that won World Series\n",
    "player_links = []\n",
    "for k in range(len(win_team_soups)):\n",
    "    team_pl_links = []\n",
    "    counter =0\n",
    "    for this_soup in win_team_soups[k].find_all('td', {\"class\": \"left\", \"data-stat\": \"player\"}):\n",
    "        try:\n",
    "            counter += 1\n",
    "            link = this_soup.find('a')['href']\n",
    "            team_pl_links.append(link)\n",
    "            if counter >= 9:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    player_links.append(team_pl_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage content for each player\n",
    "player_pages = []\n",
    "player_soups = []\n",
    "for i in range(len(player_links)):\n",
    "    if i == 1:\n",
    "        time.sleep(65) # sleep timer because we don't want to exceed 20 requests in a minute, adding 10 seconds for safety\n",
    "    temp_pages = []\n",
    "    temp_soups = []\n",
    "    for j in range(len(player_links[i])):\n",
    "        url = f'{main_url}{player_links[i][j]}'\n",
    "        page = requests.get(url)\n",
    "        temp_pages.append(page)\n",
    "        temp_soups.append(BeautifulSoup(page.content))\n",
    "        \n",
    "    player_pages.append(temp_pages)\n",
    "    player_soups.append(temp_soups)\n",
    "    \n",
    "# 27 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting links to the gamelogs for each player for the years that the players won the world series\n",
    "player_game_log_links = []\n",
    "for i in range(len(player_soups)):\n",
    "    temp = []\n",
    "    flag = 0\n",
    "    for j in range(len(player_soups[i])):\n",
    "        counter = 0\n",
    "        flag = 0\n",
    "        for search in player_soups[i][j].find_all('li', {'data-fade-selector': '#inpage_nav'}):\n",
    "            for sub_search in search.find_all('ul'):\n",
    "                counter += 1\n",
    "                for sub2_search in sub_search.find_all('li'):\n",
    "                    link = sub2_search.find('a')['href']\n",
    "                    check = re.search(f'{years[i]}', link)\n",
    "                    if (check) and (counter == 2 or counter == 3):\n",
    "                        temp.append(link)\n",
    "                        flag = 1\n",
    "                        break\n",
    "                if flag == 1:\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                break\n",
    "    player_game_log_links.append(temp)\n",
    "    \n",
    "# had to hard-code a fix to one of the links. It was an abnormality.\n",
    "player_game_log_links[0][2] = '/players/gl.fcgi?id=doziebr01&t=b&year=2019'                        \n",
    "\n",
    "# print(player_soups[2][2].find_all('li', {'data-fade-selector': '#inpage_nav'}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of the players names by year\n",
    "players = []\n",
    "for i in range(len(player_soups)):\n",
    "    players_team_array = []\n",
    "    for j in range(len(player_soups[i])):\n",
    "        for search in player_soups[i][j].find_all('div', id='meta'):\n",
    "            name = search.find('h1')\n",
    "            name = name.text.strip()\n",
    "        players_team_array.append(name)\n",
    "    players.append(players_team_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframes for team stats that won the world series\n",
    "batting_team_stats = []\n",
    "pitching_team_stats = []\n",
    "for i in range(len(win_team_pages)):\n",
    "    [batting, pitching] = pd.read_html(win_team_pages[i].content)\n",
    "    batting_team_stats.append(batting)\n",
    "    pitching_team_stats.append(pitching)\n",
    "\n",
    "# need to delete row index 8 in most (if not all) dataframes for batting. Also delete the last 5 rows.\n",
    "# delete index row 5, 11, and last 3 rows in pitching. Look at each dataframe before making edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the tables for each player that has stats for each game that they played\n",
    "player_game_stats = []\n",
    "time.sleep(65) # do not exceed 20 requests in a minute\n",
    "for i in range(len(player_game_log_links)):\n",
    "    temp = []\n",
    "    if i == 2:\n",
    "        time.sleep(65) # do not exceed 20 requests in a minute\n",
    "    for j in range(len(player_game_log_links[i])):\n",
    "        page = requests.get(f'{main_url}{player_game_log_links[i][j]}')\n",
    "        df = pd.read_html(page.content)\n",
    "        temp.append(df[4])\n",
    "    player_game_stats.append(temp)\n",
    "    \n",
    "# still need to clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
